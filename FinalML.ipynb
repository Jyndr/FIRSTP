{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN58iccxZsdYk59jmEmEqHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jyndr/FIRSTP/blob/main/FinalML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL REQUIRED PACKAGES"
      ],
      "metadata": {
        "id": "MSM-tkzpMqU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: nltk\n",
        "\n",
        "# # ALL REQUIRED PACKAGES\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KQkTeGuQd-p",
        "outputId": "0392da3f-98e9-4b3a-bf26-e9e6d62ac676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n"
      ],
      "metadata": {
        "id": "A-_5CKX6Mkxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM1D14d1Ll2o"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# prompt: required_packages = [\n",
        "#     \"requests\",\n",
        "#     \"feedparser\",\n",
        "#     \"pandas\",\n",
        "#     \"nltk\",\n",
        "#     \"beautifulsoup4\",\n",
        "#     \"scikit-learn\",\n",
        "#     \"fake-useragent\",\n",
        "#     \"joblib\",\n",
        "#     \"scipy\",\n",
        "#     \"threadpoolctl\",\n",
        "#     \"regex\",\n",
        "#     \"tqdm\"\n",
        "# ]\n",
        "\n",
        "required_packages = [\n",
        "    \"requests\",\n",
        "    \"feedparser\",\n",
        "    \"pandas\",\n",
        "    \"nltk\",\n",
        "    \"beautifulsoup4\",\n",
        "    \"scikit-learn\",\n",
        "    \"fake-useragent\",\n",
        "    \"joblib\",\n",
        "    \"scipy\",\n",
        "    \"threadpoolctl\",\n",
        "    \"regex\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "!pip install -Uqq {\" \".join(required_packages)}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def install_packages():\n",
        "    \"\"\"Install missing packages\"\"\"\n",
        "    print(\"Checking and installing required packages...\")\n",
        "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "    missing = [pkg for pkg in required_packages if pkg.lower() not in installed]\n",
        "\n",
        "    if missing:\n",
        "        print(f\"Installing missing packages: {', '.join(missing)}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
        "        print(\"All required packages installed successfully.\")\n",
        "    else:\n",
        "        print(\"All required packages already installed.\")\n"
      ],
      "metadata": {
        "id": "Z7zulTx8MCRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "install_packages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t2AEPacMVk9",
        "outputId": "0ce2cba9-ea31-4cc8-9043-c43ebfb047d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking and installing required packages...\n",
            "All required packages already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT"
      ],
      "metadata": {
        "id": "1AvofqcuNHjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import ssl\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from fake_useragent import UserAgent\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "PnaAt2rGNNQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssl._create_default_https_context = ssl._create_unverified_context\n"
      ],
      "metadata": {
        "id": "wZl2RQ-jNcob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_model = None\n",
        "vectorizer = None"
      ],
      "metadata": {
        "id": "-jVBnqmENhbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED_NmCHhNpCX",
        "outputId": "fe9a3cfe-c89b-4bec-ebcb-6c7564da7998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DISASTER_KEYWORDS = [\n",
        "    \"disaster\", \"earthquake\", \"flood\", \"cyclone\", \"hurricane\", \"tsunami\",\n",
        "    \"wildfire\", \"forest fire\", \"landslide\", \"avalanche\", \"tornado\", \"storm\",\n",
        "    \"drought\", \"famine\", \"evacuation\", \"emergency\", \"crisis\", \"catastrophe\",\n",
        "    \"destruction\", \"devastation\", \"collapse\", \"extreme weather\", \"outbreak\",\n",
        "    \"rescue\", \"relief\", \"victims\", \"survivors\", \"damage\", \"casualties\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "4tMW3WYbNtIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsArticle:\n",
        "    def __init__(self, source, title, content, url, published_date=None):\n",
        "        self.source = source\n",
        "        self.title = title\n",
        "        self.content = content if content else \"\"\n",
        "        self.url = url\n",
        "        self.published_date = DISASTER_KEYWORDS = [published_date]\n",
        "        self.is_disaster_related = self._check_disaster_relevance()\n",
        "\n",
        "    def _check_disaster_relevance(self):\n",
        "        \"\"\"Check if content is related to disasters using keyword matching\"\"\"\n",
        "        text = f\"{self.title} {self.content}\".lower()\n",
        "        return any(keyword.lower() in text for keyword in DISASTER_KEYWORDS)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"source\": self.source,\n",
        "            \"title\": self.title,\n",
        "            \"content\": self.content,\n",
        "            \"url\": self.url,\n",
        "            \"published_date\": self.published_date,\n",
        "            \"is_disaster_related\": self.is_disaster_related\n",
        "        }\n"
      ],
      "metadata": {
        "id": "_KEv3W5XN9Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GOOGLE NEWS"
      ],
      "metadata": {
        "id": "kWQpOUMZbOmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_google_news():\n",
        "    \"\"\"Scrape Google News for disaster-related articles\"\"\"\n",
        "    print(\"Scraping Google News...\")\n",
        "    articles = []\n",
        "\n",
        "    # Multiple queries for better coverage\n",
        "    queries = [\n",
        "        \"disaster\",\n",
        "        \"earthquake+OR+tsunami\",\n",
        "        \"hurricane+OR+tornado+OR+storm\",\n",
        "        \"wildfire+OR+forest+fire\",\n",
        "        \"flood+OR+landslide\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        url = f\"https://news.google.com/rss/search?q={query}\"\n",
        "        try:\n",
        "            feed = feedparser.parse(url)\n",
        "            print(f\"Found {len(feed.entries)} articles for query: {query}\")\n",
        "\n",
        "            for entry in feed.entries:\n",
        "                article = NewsArticle(\n",
        "                    source=\"Google News\",\n",
        "                    title=entry.title,\n",
        "                    content=entry.summary if hasattr(entry, 'summary') else \"\",\n",
        "                    url=entry.link,\n",
        "                    published_date=entry.published if hasattr(entry, 'published') else None\n",
        "                )\n",
        "\n",
        "                if article.is_disaster_related:\n",
        "                    articles.append(article.to_dict())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping Google News with query '{query}': {str(e)}\")\n",
        "\n",
        "    print(f\"Retrieved {len(articles)} disaster-related Google News articles\")\n",
        "    return articles\n"
      ],
      "metadata": {
        "id": "Vi7QZ9oTOU7S"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REDDIT"
      ],
      "metadata": {
        "id": "57o_PsZjbS1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asyncpraw aiohttp requests-cache\n",
        "\n",
        "import asyncio\n",
        "import asyncpraw\n",
        "from datetime import datetime\n",
        "import aiohttp\n",
        "import requests_cache\n",
        "\n",
        "# Assuming you have the NewsArticle class and DISASTER_KEYWORDS defined\n",
        "\n",
        "# Setup caching for requests\n",
        "requests_cache.install_cache('news_cache', backend='sqlite', expire_after=3600)  # Cache for 1 hour\n",
        "\n",
        "\n",
        "async def scrape_reddit_news(reddit_client_id, reddit_client_secret, reddit_user_agent):\n",
        "    \"\"\"Scrape Reddit for disaster-related news using asyncpraw.\"\"\"\n",
        "    print(\"Scraping Reddit...\")\n",
        "\n",
        "    async for submission in subreddit.hot(limit=50):\n",
        "      async def collect_all_disaster_news():\n",
        "       \"\"\"Collect disaster news from all sources into a single combined dataset\"\"\"\n",
        "       print(\"Collecting data from all sources...\")\n",
        "       all_articles = []\n",
        "\n",
        "       # Collect from various sources, awaiting asynchronous calls\n",
        "       all_articles.extend(scrape_google_news())\n",
        "       all_articles.extend(scrape_news_websites())\n",
        "       all_articles.extend(scrape_twitter())\n",
        "       all_articles.extend(await scrape_reddit_news(reddit_client_id, reddit_client_secret, reddit_user_agent)) # Await the coroutine\n",
        "       all_articles.extend(scrape_newsapi())\n",
        "\n",
        "       # ... (rest of the function) ...\n",
        "           # ...\n",
        "    try:\n",
        "        reddit = asyncpraw.Reddit(\n",
        "            client_id=reddit_client_id,\n",
        "            client_secret=reddit_client_secret,\n",
        "            user_agent=reddit_user_agent,\n",
        "        )\n",
        "\n",
        "        articles = []\n",
        "        subreddit_names = [\"news\", \"worldnews\", \"Disaster\"]  # Add more relevant subreddits\n",
        "\n",
        "        for subreddit_name in subreddit_names:\n",
        "            try:\n",
        "                subreddit = await reddit.subreddit(subreddit_name)\n",
        "                async for submission in subreddit.hot(limit=50):  # Reduced limit for faster execution\n",
        "                    if submission.link_flair_text and submission.link_flair_text.lower() == \"disaster\":  # Case-insensitive check\n",
        "                        if submission.title and submission.url:\n",
        "                            article = NewsArticle(\n",
        "                                source=\"Reddit\",\n",
        "                                title=submission.title,\n",
        "                                content=submission.selftext,\n",
        "                                url=submission.url,\n",
        "                                published_date=datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
        "                            )\n",
        "\n",
        "                            if article.is_disaster_related:\n",
        "                                articles.append(article.to_dict())\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping subreddit '{subreddit_name}': {str(e)}\")\n",
        "\n",
        "        print(f\"Retrieved {len(articles)} disaster-related Reddit articles\")\n",
        "        return articles\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Reddit client or during scraping: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def process_and_display_results(df):\n",
        "    \"\"\"Process the collected data and prepare for display\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No disaster-related news found!\")\n",
        "        return\n",
        "\n",
        "    # Prepare final output\n",
        "    output_df = df[['source', 'title', 'url']].copy()  # Select desired columns\n",
        "    output_df.columns = ['Source', 'Title', 'URL']  # Rename columns\n",
        "\n",
        "    print(\"\\n===== DISASTER NEWS RESULTS =====\")\n",
        "    print(output_df)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    # Reddit API credentials\n",
        "    reddit_client_id = \"CKPULTrV4P-8RFxvPvrGtg\"  # Replace with your actual client ID\n",
        "    reddit_client_secret = \"e1uPpOfD5OnlWdbkIA4Nu4UX49PPRA\"  # Replace with your actual client secret\n",
        "    reddit_user_agent = \"RedditScraper:v1.0 (by u/Humble-Kiwi-1133)\"  # Replace with your user agent\n",
        "\n",
        "    # Scrape Reddit for disaster-related news\n",
        "    reddit_articles = await scrape_reddit_news(reddit_client_id, reddit_client_secret, reddit_user_agent)\n",
        "\n",
        "    # Process and display the results\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(reddit_articles)\n",
        "    process_and_display_results(df)\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "      asyncio.run(main())\n",
        "async def collect_all_disaster_news():\n",
        "       # ... other code ...\n",
        "      all_articles.extend(await scrape_reddit_news(reddit_client_id, reddit_client_secret, reddit_user_agent))  # Add 'await' here\n",
        "       # ... other code ...\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY7XifZtzSG6",
        "outputId": "2eb95d4d-4d07-4be9-c591-e210a92c7d4b"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.14)\n",
            "Requirement already satisfied: requests-cache in /usr/local/lib/python3.11/dist-packages (1.2.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (24.1.0)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: cattrs>=22.2 in /usr/local/lib/python3.11/dist-packages (from requests-cache) (24.1.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests-cache) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.22 in /usr/local/lib/python3.11/dist-packages (from requests-cache) (2.32.3)\n",
            "Requirement already satisfied: url-normalize>=1.4 in /usr/local/lib/python3.11/dist-packages (from requests-cache) (1.4.3)\n",
            "Requirement already satisfied: urllib3>=1.25.5 in /usr/local/lib/python3.11/dist-packages (from requests-cache) (2.3.0)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->requests-cache) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->requests-cache) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22->requests-cache) (2025.1.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from url-normalize>=1.4->requests-cache) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TWITTER"
      ],
      "metadata": {
        "id": "Yz-hiX4abWQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: i have api key secret and bearer token i want to scrap real time disaster related news from x post and writes but pls dont scrap now fully coz i just have acces to 100 post for free\n",
        "\n",
        "# Twitter API credential\n",
        "import requests\n",
        "\n",
        "class NewsArticle:\n",
        "    def __init__(self, source, title, content, url, published_date):\n",
        "        self.source = source\n",
        "        self.title = title\n",
        "        self.content = content\n",
        "        self.url = url\n",
        "        self.published_date = published_date\n",
        "\n",
        "    @property\n",
        "    def is_disaster_related(self):\n",
        "        # Add logic here to determine if an article is related to a disaster\n",
        "        # This could involve keyword matching or more sophisticated NLP techniques\n",
        "        return True  # Placeholder for now, replace with actual logic\n",
        "\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            \"source\": self.source,\n",
        "            \"title\": self.title,\n",
        "            \"content\": self.content,\n",
        "            \"url\": self.url,\n",
        "            \"published_date\": self.published_date\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# prompt: i have api key secret and bearer token i want to scrap real time disaster related news from x post and writes but pls dont scrap now fully coz i just have acces to 100 post for free\n",
        "\n",
        "# Twitter API credentials\n",
        "API_KEY = \"kdfwp2GOryIyxgEM9qM2Tly23\"\n",
        "API_SECRET = \"HTnkpSrxFNj6k9oVzqObtzQ8smGSaGrCrBfx3LFVQ423CJhyVX\"\n",
        "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAADyX0AEAAAAAm57yGP8lSQjcR%2F1Inc%2FeU5ZLBbM%3DL6KyExxykq45NURkxVub3IsUiJVbLYBif4iMMf6xp2z11Dxyma\"\n",
        "\n",
        "\n",
        "def scrape_twitter():\n",
        "    \"\"\"Scrape Twitter for disaster-related tweets\"\"\"\n",
        "    print(\"Scraping Twitter...\")\n",
        "    articles = []\n",
        "\n",
        "    search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "\n",
        "    # Sample query (adjust as needed)\n",
        "    query_params = {\n",
        "        'query': '(\"earthquake\" OR \"flood\" OR \"cyclone\" OR \"hurricane\" OR \"wildfire\" OR \"landslide\" OR \"tsunami\" OR \"tornado\" OR \"disaster\" OR \"volcanic eruption\") (\"breaking\" OR \"alert\" OR \"update\" OR \"news\" OR \"evacuation\" OR \"damage\" OR \"rescue\" OR \"emergency\") lang:en',\n",
        "        'max_results': '50',  # Max results per request\n",
        "        'tweet.fields': 'created_at,author_id',\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {BEARER_TOKEN}\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(search_url, headers=headers, params=query_params)\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(\n",
        "                f\"Request returned an error: {response.status_code} {response.text}\"\n",
        "            )\n",
        "\n",
        "        data = response.json()\n",
        "        tweets = data.get(\"data\", [])\n",
        "\n",
        "        print(f\"Found {len(tweets)} tweets\")\n",
        "\n",
        "        for tweet in tweets:\n",
        "            article = NewsArticle(\n",
        "                source=\"Twitter\",\n",
        "                title=tweet.get(\"text\", \"\"),\n",
        "                content=\"\",\n",
        "                url=f\"https://twitter.com/i/web/status/{tweet.get('id')}\",\n",
        "                published_date=tweet.get(\"created_at\")\n",
        "            )\n",
        "\n",
        "            if article.is_disaster_related:\n",
        "                articles.append(article.to_dict())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping Twitter: {str(e)}\")\n",
        "\n",
        "    print(f\"Retrieved {len(articles)} disaster-related Twitter tweets\")\n",
        "    return articles\n",
        ""
      ],
      "metadata": {
        "id": "qZr4N69Shn1h"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News Webs"
      ],
      "metadata": {
        "id": "kL5OTH23bbsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_news_websites():\n",
        "    \"\"\"Scrape news websites using RSS feeds\"\"\"\n",
        "    print(\"Scraping news websites...\")\n",
        "    articles = []\n",
        "\n",
        "    RSS_FEEDS = {\n",
        "        \"CNN\": \"http://rss.cnn.com/rss/cnn_world.rss\",\n",
        "        \"BBC\": \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
        "        \"The Guardian\": \"https://www.theguardian.com/world/natural-disasters/rss\",\n",
        "        \"Reuters\": \"http://feeds.reuters.com/reuters/worldNews\",\n",
        "        \"NPR\": \"https://feeds.npr.org/1001/rss.xml\"\n",
        "    }\n",
        "\n",
        "    for source, url in RSS_FEEDS.items():\n",
        "        try:\n",
        "            print(f\"Checking {source}...\")\n",
        "            feed = feedparser.parse(url)\n",
        "\n",
        "            for entry in feed.entries:\n",
        "                title = entry.title if hasattr(entry, 'title') else \"\"\n",
        "                content = entry.summary if hasattr(entry, 'summary') else \"\"\n",
        "                link = entry.link if hasattr(entry, 'link') else \"\"\n",
        "                published = entry.published if hasattr(entry, 'published') else None\n",
        "\n",
        "                article = NewsArticle(\n",
        "                    source=source,\n",
        "                    title=title,\n",
        "                    content=content,\n",
        "                    url=link,\n",
        "                    published_date=published\n",
        "                )\n",
        "\n",
        "                if article.is_disaster_related:\n",
        "                    articles.append(article.to_dict())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {source}: {str(e)}\")\n",
        "\n",
        "    print(f\"Retrieved {len(articles)} disaster-related news website articles\")\n",
        "    return articles"
      ],
      "metadata": {
        "id": "cbTZLNU5OnY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News API"
      ],
      "metadata": {
        "id": "PPzrHEipbgTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_newsapi(api_key=\"51a6f20aff2f4db8ba764f07af32f59f\"):\n",
        "    \"\"\"Fetch disaster news from NewsAPI\"\"\"\n",
        "    print(\"Fetching from NewsAPI...\")\n",
        "    articles = []\n",
        "\n",
        "    if not api_key:\n",
        "        print(\"NewsAPI key not provided, skipping this source\")\n",
        "        return articles\n",
        "\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    query = \" OR \".join(DISASTER_KEYWORDS[:10])  # Use first 10 keywords\n",
        "\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"language\": \"en\",\n",
        "        \"sortBy\": \"publishedAt\",\n",
        "        \"apiKey\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        data = response.json()\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"NewsAPI error: {data.get('message', 'Unknown error')}\")\n",
        "            return articles\n",
        "\n",
        "        if \"articles\" not in data:\n",
        "            print(\"No articles found in NewsAPI response\")\n",
        "            return articles\n",
        "\n",
        "        print(f\"Found {len(data['articles'])} articles from NewsAPI\")\n",
        "\n",
        "        for article_data in data[\"articles\"]:\n",
        "            article = NewsArticle(\n",
        "                source=article_data[\"source\"][\"name\"],\n",
        "                title=article_data[\"title\"],\n",
        "                content=article_data[\"description\"] if article_data.get(\"description\") else \"\",\n",
        "                url=article_data[\"url\"],\n",
        "                published_date=article_data[\"publishedAt\"]\n",
        "            )\n",
        "\n",
        "            if article.is_disaster_related:\n",
        "                articles.append(article.to_dict())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching from NewsAPI: {str(e)}\")\n",
        "\n",
        "    print(f\"Retrieved {len(articles)} disaster-related NewsAPI articles\")\n",
        "    return articles\n"
      ],
      "metadata": {
        "id": "JAbmLhQiOs5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# ... other imports ...\n",
        "\n",
        "# Download the 'punkt_tab' package before using word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def verify_disaster_relevance(text):\n",
        "    \"\"\"Verify if text is related to disasters using NLP\"\"\"\n",
        "    # Tokenize and clean the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Check for disaster keywords\n",
        "    for keyword in DISASTER_KEYWORDS:\n",
        "        if keyword.lower() in tokens or keyword.lower() in text:\n",
        "            return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vuzjlX0O5Lz",
        "outputId": "e0b11786-b064-45e5-c265-990876f5de74"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Vm-orAFGbszf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fake_news_detection_model():\n",
        "    \"\"\"Build and train a model to detect fake news\"\"\"\n",
        "    print(\"Building fake news detection model...\")\n",
        "\n",
        "    # Prepare training data\n",
        "    # For demo purposes, we'll create a simple dataset with synthetic labels\n",
        "    # In a real implementation, you would use an actual labeled dataset\n",
        "\n",
        "    # Collect some news articles first\n",
        "    articles = []\n",
        "    articles.extend(scrape_google_news())\n",
        "    articles.extend(scrape_news_websites())\n",
        "\n",
        "    if not articles:\n",
        "        print(\"Warning: Could not gather enough articles to train model\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.DataFrame(articles)\n",
        "\n",
        "    # Create labeled data (in a real scenario, you would have actual labeled data)\n",
        "    # For now, we'll use heuristics to create synthetic labels\n",
        "\n",
        "    # Mainstream sources are more likely to be real (but not guaranteed)\n",
        "    mainstream_sources = ['CNN', 'BBC', 'Reuters', 'Associated Press',\n",
        "                          'The Guardian', 'NPR', 'New York Times',\n",
        "                          'Washington Post', 'ABC', 'CBS', 'NBC']\n",
        "\n",
        "    # Assign 'real' label to mainstream sources and longer content articles\n",
        "    df['label'] = 'fake'  # Default\n",
        "\n",
        "    # Mark as real if from mainstream source\n",
        "    df.loc[df['source'].apply(lambda x: any(s in x for s in mainstream_sources)), 'label'] = 'real'\n",
        "\n",
        "    # Mark as real if content is substantial\n",
        "    df.loc[df['content'].apply(lambda x: len(str(x)) > 500), 'label'] = 'real'\n",
        "\n",
        "    # Random label assignment to create more balanced dataset\n",
        "    # Assign 'fake' label to 20% of the 'real' labeled articles randomly\n",
        "    real_indices = df[df['label'] == 'real'].index.tolist()\n",
        "    import random\n",
        "    fake_indices = random.sample(real_indices, k=int(0.2 * len(real_indices)))\n",
        "    df.loc[fake_indices, 'label'] = 'fake'\n",
        "\n",
        "    # Combine title and content for better features\n",
        "    df['text'] = df['title'] + ' ' + df['content'].fillna('')\n",
        "\n",
        "    # Split into training and test sets\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X = df['text']\n",
        "    y = df['label']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Feature extraction using TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the model (Logistic Regression for simplicity and interpretability)\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    from sklearn.metrics import accuracy_score, classification_report\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return model, tfidf_vectorizer\n"
      ],
      "metadata": {
        "id": "luaOvKlfO_Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trust Score"
      ],
      "metadata": {
        "id": "yy_MCmLZbw2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_trust_score(article):\n",
        "    \"\"\"Calculate a trust score for an article using ML model\"\"\"\n",
        "    global ml_model, vectorizer\n",
        "\n",
        "    # If model not loaded, use fallback method\n",
        "    if ml_model is None or vectorizer is None:\n",
        "        # Fallback to a simpler heuristic if model is not available\n",
        "        return fallback_calculate_trust_score(article)\n",
        "\n",
        "    # Prepare the text for prediction\n",
        "    text = f\"{article['title']} {article['content']}\" if article['content'] else article['title']\n",
        "\n",
        "    try:\n",
        "        # Transform text using vectorizer\n",
        "        text_tfidf = vectorizer.transform([text])\n",
        "\n",
        "        # Get probability of being 'real'\n",
        "        probabilities = ml_model.predict_proba(text_tfidf)[0]\n",
        "        real_idx = ml_model.classes_.tolist().index('real')\n",
        "        trust_score = probabilities[real_idx]\n",
        "\n",
        "        # Ensure score is between 0 and 1\n",
        "        return max(0.1, min(0.95, trust_score))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting trust score: {str(e)}\")\n",
        "        return fallback_calculate_trust_score(article)\n",
        "\n",
        "# Fallback trust score calculation using heuristics\n",
        "def fallback_calculate_trust_score(article):\n",
        "    \"\"\"Fallback method to calculate trust score when ML model is unavailable\"\"\"\n",
        "    # Base score between 0.6 and 0.9\n",
        "    base_score = 0.6\n",
        "\n",
        "    # List of trusted mainstream sources\n",
        "    mainstream_sources = ['CNN', 'BBC', 'Reuters', 'Associated Press',\n",
        "                          'The Guardian', 'NPR', 'New York Times',\n",
        "                          'Washington Post', 'ABC', 'CBS', 'NBC']\n",
        "\n",
        "    # Bonus for mainstream sources\n",
        "    if any(source in article['source'] for source in mainstream_sources):\n",
        "        base_score += 0.2\n",
        "\n",
        "    # Bonus for having a valid URL\n",
        "    if article['url'] and ('http' in article['url']):\n",
        "        base_score += 0.1\n",
        "\n",
        "    # Check content length - higher quality articles tend to be longer\n",
        "    content_length = len(article['content']) if article['content'] else 0\n",
        "    if content_length > 500:\n",
        "        base_score += 0.05\n",
        "\n",
        "    # Cap at 0.95\n",
        "    return min(base_score, 0.95)\n"
      ],
      "metadata": {
        "id": "DmOPufX_PJ8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collecting"
      ],
      "metadata": {
        "id": "-6ZW2W0Gb0zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_all_disaster_news():\n",
        "    \"\"\"Collect disaster news from all sources into a single combined dataset\"\"\"\n",
        "    print(\"Collecting data from all sources...\")\n",
        "    all_articles = []\n",
        "\n",
        "    # Collect from various sources\n",
        "    all_articles.extend(scrape_google_news())\n",
        "    all_articles.extend(scrape_news_websites())\n",
        "    all_articles.extend(scrape_twitter())\n",
        "    all_articles.extend(scrape_reddit())\n",
        "    all_articles.extend(scrape_newsapi())\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_articles)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"No articles found from any source!\")\n",
        "        return df\n",
        "\n",
        "    print(f\"Collected {len(df)} raw articles from all sources\")\n",
        "\n",
        "    # Clean and process the combined dataset\n",
        "    df = clean_combined_dataset(df)\n",
        "\n",
        "    # Calculate trust scores using ML model\n",
        "    df = calculate_trust_scores_for_dataset(df)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "03XClh2wPYsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "tD2JKCN_b7xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_combined_dataset(df):\n",
        "    \"\"\"Clean and preprocess the combined dataset\"\"\"\n",
        "    print(\"Cleaning and preprocessing combined dataset...\")\n",
        "\n",
        "    # Create a copy to avoid SettingWithCopyWarning\n",
        "    df = df.copy()  # Create an explicit copy\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Remove duplicates based on URL\n",
        "    initial_count = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"url\"])\n",
        "    print(f\"Removed {initial_count - len(df)} duplicate articles\")\n",
        "\n",
        "    # Ensure all required columns exist\n",
        "    for col in [\"source\", \"title\", \"content\", \"url\", \"published_date\"]:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "\n",
        "    # Clean text data\n",
        "    df.loc[:, 'title'] = df['title'].fillna('').astype(str).apply(clean_text)\n",
        "    df.loc[:, 'content'] = df['content'].fillna('').astype(str).apply(clean_text)\n",
        "\n",
        "    # Add timestamp for real-time tracking\n",
        "    df.loc[:, 'timestamp'] = datetime.now().isoformat()\n",
        "\n",
        "    # Define the 'classify_disaster' function within 'clean_combined_dataset'\n",
        "    def classify_disaster(title, content):\n",
        "        \"\"\"Classify if an article is related to a disaster.\"\"\"\n",
        "        text = f\"{title} {content}\".lower()\n",
        "        return any(keyword.lower() in text for keyword in DISASTER_KEYWORDS)\n",
        "\n",
        "    # Double-verify disaster relevance using NLP\n",
        "    df.loc[:, \"is_disaster_related\"] = df.apply(\n",
        "    lambda row: classify_disaster(row['title'], row['content']), axis=1\n",
        ")\n",
        "\n",
        "    # Filter to keep only disaster-related content\n",
        "    df_filtered = df[df[\"is_disaster_related\"]]\n",
        "    print(f\"After cleaning: {len(df_filtered)} disaster-related articles from {len(df)} total\")\n",
        "    return df_filtered"
      ],
      "metadata": {
        "id": "Foyu8mylZzW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text data\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = ' '.join(word for word in text.split() if not word.startswith('http'))\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "3O8tBbqePrNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_trust_scores_for_dataset(df):\n",
        "    \"\"\"Calculate trust scores for the entire dataset using ML model\"\"\"\n",
        "    print(\"Calculating trust scores using ML model...\")\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Calculate trust scores\n",
        "    df[\"trust_score\"] = df.apply(\n",
        "        lambda row: calculate_trust_score(row),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "pmdJDoc-PvXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_display_results(df):\n",
        "    \"\"\"Process the collected data and prepare for display\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"No disaster-related news found!\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Convert trust_score to percentage\n",
        "    df['trust_score_percent'] = (df['trust_score'] * 100).round(1)\n",
        "\n",
        "    # Prepare final output\n",
        "    output_df = df[['source', 'title', 'url', 'trust_score_percent']].copy()\n",
        "    output_df.columns = ['Source', 'Title', 'URL', 'Trust Score (%)']\n",
        "\n",
        "    print(\"\\n===== DISASTER NEWS RESULTS =====\")\n",
        "    print(output_df.head(10))\n",
        "    return output_df\n"
      ],
      "metadata": {
        "id": "1meKoIJRP0y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REAL Time"
      ],
      "metadata": {
        "id": "7TdLh4i8cOR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getreal_time_disaster_news():\n",
        "    \"\"\"API endpoint for web backend to get real-time disaster news\"\"\"\n",
        "    print(\"Getting real-time disaster news...\")\n",
        "\n",
        "    # Ensure ML model is loaded\n",
        "    global ml_model, vectorizer\n",
        "    if ml_model is None or vectorizer is None:\n",
        "        ml_model, vectorizer = build_fake_news_detection_model()\n",
        "\n",
        "    # Get fresh data from all sources\n",
        "    df_news = collect_all_disaster_news()\n",
        "\n",
        "    if df_news.empty:\n",
        "        return []\n",
        "\n",
        "    # Process for display\n",
        "    results = process_and_display_results(df_news)\n",
        "\n",
        "    # Convert to format for web display with timestamp\n",
        "    current_time = datetime.now().isoformat()\n",
        "    news_items = []\n",
        "    for _, row in results.iterrows():\n",
        "        news_items.append({\n",
        "            'source': row['Source'],\n",
        "            'title': row['Title'],\n",
        "            'url': row['URL'],\n",
        "            'trust_score': row['Trust Score (%)'],\n",
        "            'retrieved_at': current_time\n",
        "        })\n",
        "\n",
        "    return news_items"
      ],
      "metadata": {
        "id": "h7zsWQ_kP7-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "buSZm075cefW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now i want to extract the real time news and ready to send predicted processed reduklt\n",
        "\n",
        "def send_prediction_results(results):\n",
        "    \"\"\"Sends the predicted processed results to a designated endpoint.\"\"\"\n",
        "    print(\"Sending prediction results to endpoint...\")\n",
        "    # In a real implementation, you'd have a specific API or messaging system\n",
        "    # Here, we'll just print the results to the console as an example\n",
        "    print(json.dumps(results, indent=2))\n",
        "\n",
        "def main():\n",
        "    # ... (rest of your code) ...\n",
        "\n",
        "    # Build ML model for fake news detection\n",
        "    global ml_model, vectorizer\n",
        "    ml_model, vectorizer = build_fake_news_detection_model()\n",
        "\n",
        "    # Collect and process all disaster news\n",
        "    df = collect_all_disaster_news()\n",
        "\n",
        "    # Process and display results\n",
        "    processed_df = process_and_display_results(df)\n",
        "\n",
        "    # Send predicted processed results\n",
        "    if not processed_df.empty:\n",
        "        results = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            results.append({\n",
        "                'source': row['Source'],\n",
        "                'title': row['Title'],\n",
        "                'url': row['URL'],\n",
        "                'trust_score': row['Trust Score (%)']\n",
        "            })\n",
        "        send_prediction_results(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "8go7oSFZTVz0",
        "outputId": "8550c101-3e8a-4dc0-eb14-80e1aff89cd0"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building fake news detection model...\n",
            "Scraping Google News...\n",
            "Found 100 articles for query: disaster\n",
            "Found 100 articles for query: earthquake+OR+tsunami\n",
            "Found 100 articles for query: hurricane+OR+tornado+OR+storm\n",
            "Found 102 articles for query: wildfire+OR+forest+fire\n",
            "Found 99 articles for query: flood+OR+landslide\n",
            "Retrieved 501 disaster-related Google News articles\n",
            "Scraping news websites...\n",
            "Checking CNN...\n",
            "Checking BBC...\n",
            "Checking The Guardian...\n",
            "Checking Reuters...\n",
            "Checking NPR...\n",
            "Retrieved 83 disaster-related news website articles\n",
            "Model accuracy: 0.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.57      0.98      0.72        64\n",
            "        real       0.86      0.11      0.20        53\n",
            "\n",
            "    accuracy                           0.59       117\n",
            "   macro avg       0.71      0.55      0.46       117\n",
            "weighted avg       0.70      0.59      0.49       117\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'coroutine' object has no attribute 'empty'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-761283927226>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-167-761283927226>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Process and display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_and_display_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Send predicted processed results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-166-2eb3af6e7146>\u001b[0m in \u001b[0;36mprocess_and_display_results\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_and_display_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m\"\"\"Process the collected data and prepare for display\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No disaster-related news found!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'coroutine' object has no attribute 'empty'"
          ]
        }
      ]
    }
  ]
}